{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic  \\\n",
       "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...           1.0   \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...           1.0   \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...           0.0   \n",
       "3                                       نہیں پائین 😎           0.0   \n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...           1.0   \n",
       "\n",
       "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \n",
       "0         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "1         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "2         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "3         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "4         NaN         NaN         NaN         NaN        NaN         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'urdu_sarcastic_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['آئی', 'آئے', 'آج', 'آخر', 'آخرکبر', 'آدهی', 'آًب', 'آٹھ', 'آیب', 'اة']\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords from the text file\n",
    "with open('stopwords-ur.txt', 'r', encoding='utf-8') as file:\n",
    "    urdu_stopwords = file.read().splitlines()\n",
    "\n",
    "# Check the first few stopwords\n",
    "print(urdu_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>🤣😂😂 لینے میری شادی فسادن کوجی نہیں چاہیے 😐😐😐🤣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی اپوزیش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>`` مراد علی شاہ بھیس میں ڈی جی ایس '' حامد میر😁</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
       "3                                       نہیں پائین 😎   \n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0      🤣😂😂 لینے میری شادی فسادن کوجی نہیں چاہیے 😐😐😐🤣  \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...  \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی اپوزیش...  \n",
       "3                                       نہیں پائین 😎  \n",
       "4    `` مراد علی شاہ بھیس میں ڈی جی ایس '' حامد میر😁  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove stopwords, handling non-string values\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        words = text.split()  # Split text into words\n",
    "        cleaned_text = ' '.join([word for word in words if word not in urdu_stopwords])\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return text  # Return the text unchanged if it's not a string\n",
    "\n",
    "# Apply the stopword removal to the 'urdu_text' column\n",
    "df['cleaned_text'] = df['urdu_text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the cleaned text\n",
    "df[['urdu_text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Punctuation, Emojis, and Hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td>نہیں پائین</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
       "3                                       نہیں پائین 😎   \n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...  \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...  \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...  \n",
       "3                                         نہیں پائین  \n",
       "4  مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 1. Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # 2. Remove Hashtags\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        \n",
    "        # 3. Remove Punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # 4. Remove all emojis\n",
    "        text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FAFF]', '', text)\n",
    "\n",
    "        return text.strip()\n",
    "    else:\n",
    "        return text  # Return unchanged if not a string\n",
    "\n",
    "# Apply the clean_text function to the 'urdu_text' column\n",
    "df['cleaned_text'] = df['urdu_text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "df[['urdu_text', 'cleaned_text']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Short Conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
       "3                                       نہیں پائین 😎   \n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...  \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...  \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...  \n",
       "3                                                     \n",
       "4  مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter short conversations\n",
    "def filter_short_conversations(text):\n",
    "    if isinstance(text, str):\n",
    "        # Count the number of words\n",
    "        word_count = len(text.split())\n",
    "        # Return the text if it has 3 or more words, otherwise return an empty string\n",
    "        return text if word_count >= 3 else ''\n",
    "    return text  # Return unchanged if not a string\n",
    "\n",
    "# Apply the filter_short_conversations function to the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(filter_short_conversations)\n",
    "\n",
    "# Save the cleaned text to a separate CSV file\n",
    "df[['cleaned_text']].to_csv('cleaned_urdu_text.csv', index=False, encoding='utf-8')\n",
    "# Display the results\n",
    "df[['urdu_text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Some additional preprocessing techniques (normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...\n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...\n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...\n",
       "3                                                   \n",
       "4  مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given text str, replace one or more spacings with a single space, \n",
    "# and one or more linebreaks with a single newline. \n",
    "\n",
    "from urduhack.preprocessing import normalize_whitespace\n",
    "\n",
    "# Apply normalization to the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: normalize_whitespace(x) if pd.notnull(x) else x)\n",
    "\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the normalized text\n",
    "df[['cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...\n",
      "1    چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...\n",
      "2    کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...\n",
      "3                                                     \n",
      "4    مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removing some of the additional urdu punctuations from the text\n",
    "\n",
    "from urduhack.preprocessing import remove_punctuation\n",
    "\n",
    "# Ensure all entries in 'cleaned_text' are strings. Replace non-string entries with an empty string.\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: str(x) if isinstance(x, str) else '')\n",
    "\n",
    "# Apply remove_punctuation function\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_punctuation)\n",
    "\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 20 rows of 'cleaned_text'\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...\n",
      "1    چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...\n",
      "2    کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...\n",
      "3                                                     \n",
      "4    مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove accents from any accented unicode characters in text str, either by transforming them \n",
    "# into ascii equivalents or removing them entirely.\n",
    "\n",
    "from urduhack.preprocessing import remove_accents\n",
    "\n",
    "# Ensure all entries in 'cleaned_text' are strings. Replace non-string entries with an empty string.\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: str(x) if isinstance(x, str) else '')\n",
    "\n",
    "# Apply remove_punctuation function\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_accents)\n",
    "\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 20 rows of 'cleaned_text'\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...\n",
      "1    چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...\n",
      "2    کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...\n",
      "3                                                     \n",
      "4    مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی تھے...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# This function replaces English digits with Urdu digits.\n",
    "from LughaatNLP import LughaatNLP\n",
    "\n",
    "# Initialize the LughaatNLP object\n",
    "urdu_text_processing = LughaatNLP()\n",
    "\n",
    "# Apply the replace_digits function to each row in the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: urdu_text_processing.replace_digits(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the result 'cleaned_text'\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہیں چ...\n",
      "1    چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...\n",
      "2    کامران خان اپکی دن بھریہ زمہ داری لگائی گئی اپ...\n",
      "3                                                     \n",
      "4    مراد علی شاہ کے بھیس میں ڈی جی ائی ایس ائی تھے...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# This function removes all non-Urdu characters, numbers, and special characters, just leaving only pure \n",
    "# Urdu text even not special character used in urdu.\n",
    "\n",
    "# Apply the just_urdu function to each row in the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: urdu_text_processing.just_urdu(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the result 'cleaned_text'\n",
    "print(df['cleaned_text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m spell_checker \u001b[38;5;241m=\u001b[39m LughaatNLP()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Apply the spell_check function to each row in the 'cleaned_text' column\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mspell_checker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrected_sentence_spelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save the updated data to a new CSV file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/NLP/Automatic-Sentiment-Analysis-Tool-for-Urdu-Text/nlp_assignment/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP/Automatic-Sentiment-Analysis-Tool-for-Urdu-Text/nlp_assignment/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP/Automatic-Sentiment-Analysis-Tool-for-Urdu-Text/nlp_assignment/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Desktop/NLP/Automatic-Sentiment-Analysis-Tool-for-Urdu-Text/nlp_assignment/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m spell_checker \u001b[38;5;241m=\u001b[39m LughaatNLP()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Apply the spell_check function to each row in the 'cleaned_text' column\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mspell_checker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrected_sentence_spelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save the updated data to a new CSV file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/NLP/Automatic-Sentiment-Analysis-Tool-for-Urdu-Text/nlp_assignment/lib/python3.8/site-packages/LughaatNLP/LughaatNLP.py:306\u001b[0m, in \u001b[0;36mLughaatNLP.corrected_sentence_spelling\u001b[0;34m(self, input_word, threshold)\u001b[0m\n\u001b[1;32m    304\u001b[0m max_similarity \u001b[38;5;241m=\u001b[39m (word, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vocab_word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_words:\n\u001b[0;32m--> 306\u001b[0m     edit_distance \u001b[38;5;241m=\u001b[39m \u001b[43mLevenshtein\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word), \u001b[38;5;28mlen\u001b[39m(vocab_word))\n\u001b[1;32m    308\u001b[0m     similarity_percentage \u001b[38;5;241m=\u001b[39m ((max_length \u001b[38;5;241m-\u001b[39m edit_distance) \u001b[38;5;241m/\u001b[39m max_length) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/NLP/Automatic-Sentiment-Analysis-Tool-for-Urdu-Text/nlp_assignment/lib/python3.8/site-packages/Levenshtein/__init__.py:123\u001b[0m, in \u001b[0;36mdistance\u001b[0;34m(s1, s2, weights, processor, score_cutoff, score_hint)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistance\u001b[39m(s1, s2, \u001b[38;5;241m*\u001b[39m, weights\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), processor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, score_cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, score_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    Calculates the minimum number of insertions, deletions, and substitutions\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    required to change one sequence into the other according to Levenshtein with custom\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_Levenshtein\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_cutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# spell checker\n",
    "\n",
    "# Initialize the LughaatNLP object\n",
    "spell_checker = LughaatNLP()\n",
    "\n",
    "# Apply the spell_check function to each row in the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: spell_checker.corrected_sentence_spelling(x, 60) if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the result 'cleaned_text'\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ہو لینہ دہ میری شادی فسادن ٹھیک ہہ کوجی نہا چاہیہ\n",
      "1    چل مہمانا ما کھانا سرو کر چڑیل چاچی نا دسدی اں ما\n",
      "2    کامران خان اپکی دن بھریہ زمہ داری لگائی گئی اپ...\n",
      "3                                                     \n",
      "4    مراد علی شاہ کہ بھیس ما ڈی جی ائی ایس ائی تھہ ...\n",
      "Name: stemmed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import LughaatNLP for stemming and lemmatization\n",
    "\n",
    "# Function to apply stemming only\n",
    "def apply_stemming(text):\n",
    "    if isinstance(text, str):\n",
    "        # Apply stemming\n",
    "        stemmed_text = urdu_text_processing.urdu_stemmer(text)\n",
    "        return stemmed_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply stemming to the 'cleaned_text' column\n",
    "df['stemmed_text'] = df['cleaned_text'].apply(apply_stemming)\n",
    "\n",
    "# Save the stemmed output to a new CSV file\n",
    "df[['stemmed_text']].to_csv('stemmed_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 20 rows of 'stemmed_text'\n",
    "print(df['stemmed_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ہونا لینا دینا میرا شادی فسادن ٹھیک ہونا کوجی ...\n",
      "1    چلنا مہمان میں کھا سرا کرنا چڑیل چاچی نوں دسدی...\n",
      "2    کامران خان اپکی دن بھریہ زمہ داری لگنا جانا اپ...\n",
      "3                                                     \n",
      "4    مراد علی شاہ کم بھیس میں ڈی جینا ائی ایس ائی ت...\n",
      "Name: lemmatized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to apply lemmatization only\n",
    "def apply_lemmatization(text):\n",
    "    if isinstance(text, str):\n",
    "        # Apply lemmatization\n",
    "        lemmatized_text = urdu_text_processing.lemmatize_sentence(text)\n",
    "        return lemmatized_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply lemmatization to the 'cleaned_text' column\n",
    "df['lemmatized_text'] = df['cleaned_text'].apply(apply_lemmatization)\n",
    "\n",
    "# Save the lemmatized output to a new CSV file\n",
    "df[['lemmatized_text']].to_csv('lemmatized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 20 rows of 'lemmatized_text'\n",
    "print(df['lemmatized_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [ہونا, لینا, دینا, میرا, شادی, فسادن, ٹھیک, ہو...\n",
      "1    [چلنا, مہمان, میں, کھا, سرا, کرنا, چڑیل, چاچی,...\n",
      "2    [کامران, خان, اپکی, دن, بھریہ, زمہ, داری, لگنا...\n",
      "3                                                   []\n",
      "4    [مراد, علی, شاہ, کم, بھیس, میں, ڈی, جینا, ائی,...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# This function tokenizes the Urdu text into individual tokens\n",
    "\n",
    "# Apply the urdu_tokenize function to each row in the 'lemmatized_text' column\n",
    "df['tokenized_text'] = df['lemmatized_text'].apply(\n",
    "    lambda x: urdu_text_processing.urdu_tokenize(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Save the tokenized data to a new CSV file\n",
    "df[['tokenized_text']].to_csv('tokenized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 5 rows of the tokenized text\n",
    "print(df['tokenized_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tf-IDF (Term Frequency-Inverse Document Frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
